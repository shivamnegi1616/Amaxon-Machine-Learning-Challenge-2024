{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/taruntejaneurips23/66e31d6ee96cd_student_resource_3/716AQpAJjZL.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(path)\n",
    "# # Rotate the image by 90 degrees\n",
    "# rotated_image = image.rotate(-360, expand=True)\n",
    "# # Save the rotated image\n",
    "# rotated_image.save('rotated_image.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the reader object\n",
    "reader = easyocr.Reader(['en'])  # Specify language\n",
    "\n",
    "\n",
    "results = reader.readtext(path)\n",
    "\n",
    "# Display extracted text\n",
    "for (bbox, text, prob) in results:\n",
    "    print(f\"Detected Text: {text} (Confidence: {prob:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '/home/taruntejaneurips23/66e31d6ee96cd_student_resource_3/student_resource 3/images/71Qk6hR9-WL.jpg'\n",
    "\n",
    "\n",
    "# Initialize the reader object\n",
    "reader = easyocr.Reader(['en'])  # Specify language\n",
    "\n",
    "results = reader.readtext(x)\n",
    "\n",
    "# Display extracted text\n",
    "for (bbox, text, prob) in results:\n",
    "    print(f\"Detected Text: {text} (Confidence: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(x)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import re\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the reader object\n",
    "reader = easyocr.Reader(['en'])  # Specify language\n",
    "image_path = '/home/taruntejaneurips23/66e31d6ee96cd_student_resource_3/81njuNSPdjL.jpg'\n",
    "\n",
    "# Read the text from the image\n",
    "results = reader.readtext(image_path)\n",
    "\n",
    "# Display extracted text and draw bounding boxes on image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "for (bbox, text, prob) in results:\n",
    "    print(f\"Detected Text: {text} (Confidence: {prob:.2f})\")\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    top_left = tuple([int(val) for val in bbox[0]])\n",
    "    bottom_right = tuple([int(val) for val in bbox[2]])\n",
    "    cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "# Define regex patterns\n",
    "weight_pattern = re.compile(r'(\\d+(\\.\\d+)?\\s?(g|kg))')  # Matches values like 190g, 1.5kg\n",
    "power_pattern = re.compile(r'(\\d+(\\.\\d+)?\\s?W)')  # Matches values like 50W, 1000W\n",
    "length_pattern = re.compile(r'(\\d+(\\.\\d+)?\\s?(cm|m|mm))')  # Matches lengths like 15cm, 1.2m\n",
    "width_pattern = re.compile(r'(\\d+(\\.\\d+)?\\s?(cm|m|mm))')  # Matches widths like 30cm, 0.5m\n",
    "\n",
    "# Search for patterns in the extracted text\n",
    "for _, text, _ in results:\n",
    "    weight_match = weight_pattern.search(text)\n",
    "    power_match = power_pattern.search(text)\n",
    "    length_match = length_pattern.search(text)\n",
    "    width_match = width_pattern.search(text)\n",
    "\n",
    "    if weight_match:\n",
    "        print(f\"Weight: {weight_match.group()}\")\n",
    "    if power_match:\n",
    "        print(f\"Power: {power_match.group()}\")\n",
    "    if length_match:\n",
    "        print(f\"Length: {length_match.group()}\")\n",
    "    if width_match:\n",
    "        print(f\"Width: {width_match.group()}\")\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://m.media-amazon.com/images/I/11TU2clswzL.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Initialize the reader object\n",
    "reader = easyocr.Reader(['en'])  # Specify language\n",
    "\n",
    "results = reader.readtext(image)\n",
    "# Display extracted text\n",
    "for (bbox, text, prob) in results:\n",
    "    print(f\"Detected Text: {text} (Confidence: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = 'Text information of image: '\n",
    "\n",
    "for (bbox, text, prob) in results:\n",
    "    new_prompt += f\"Detected Text: {text}, Confidence: {prob:.2f}\" + \" and \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/taruntejaneurips23/student_resource_3/81njuNSPdjL.jpg'\n",
    "image = Image.open(path)\n",
    "\n",
    "prompt = \"USER: <image>\\nWhat's the weight mention in the image? ASSISTANT:\"\n",
    "# url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"USER: <image>\\nWhat's the width in the image? ASSISTANT:\"\n",
    "url = \"https://m.media-amazon.com/images/I/11TU2clswzL.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"USER: <image>\\n {new_prompt} what's the width in the image? ASSISTANT:\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/taruntejaneurips23/student_resource_3/81njuNSPdjL.jpg'\n",
    "# image = Image.open(path)\n",
    "\n",
    "url = \"https://m.media-amazon.com/images/I/11TU2clswzL.jpg\"\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
    "\n",
    "\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# Extract relevant entity value\n",
    "print(f\"Extracted: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt('width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://m.media-amazon.com/images/I/11TU2clswzL.jpg\"\n",
    "entity_name = 'width'\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt based on the entity\n",
    "prompt = new_prompt + \"Find width value in the image?\" # generate_prompt(entity_name)\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# Extract relevant entity value\n",
    "print(f\"Extracted {entity_name}: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_from_easyocr_llava(url, entity_name):\n",
    "\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    # Easy OCR Text form the image\n",
    "    reader = easyocr.Reader(['en'])  # Specify language\n",
    "    results = reader.readtext(image)\n",
    "\n",
    "\n",
    "    new_prompt = 'Text information of image: '\n",
    "\n",
    "    for (bbox, text, prob) in results:\n",
    "        new_prompt += f\"Detected Text: {text}, Confidence: {prob:.2f}\" + \" and \" \n",
    "\n",
    "    new_prompt += f\"What is the {entity_name} in the image and given text information of that image?\"\n",
    "\n",
    "    prompt = f\"USER: <image>\\n {new_prompt} ASSISTANT:\"\n",
    "\n",
    "    # prompt = f\"USER: <image>\\n What is {entity_name} of the object in the image? ASSISTANT:\"\n",
    "\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
    "\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    print(f\"Extracted: {response}\")\n",
    "    print('=')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_from_easyocr_llava('https://m.media-amazon.com/images/I/51BhWaJFENL.jpg', entity_name = 'maximum_weight_recommendation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://m.media-amazon.com/images/I/51BhWaJFENL.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FInal Submission Using EASYOCR+Llava Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/taruntejaneurips23/student_resource_3/student_resource/dataset/test.csv'\n",
    "\n",
    "test_data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['index'][131182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {'answer' : [],\n",
    "       'prediction': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loding the models\n",
    "\n",
    "import easyocr\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the entity-unit map\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram','kilogram','microgram','milligram','ounce','pound','ton'},\n",
    "    'maximum_weight_recommendation': {'gram','kilogram','microgram','milligram','ounce','pound','ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre','cubic foot','cubic inch','cup','decilitre','fluid ounce','gallon','imperial gallon','litre','microlitre','millilitre','pint','quart'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_from_easyocr_llava(url, entity_name):\n",
    "\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    # Easy OCR Text form the image\n",
    "    reader = easyocr.Reader(['en'])  # Specify language\n",
    "    results = reader.readtext(image)\n",
    "\n",
    "    new_prompt = 'Text information of image: '\n",
    "\n",
    "    for (bbox, text, prob) in results:\n",
    "        new_prompt += text + ' '\n",
    "\n",
    "    new_prompt += f\"Find the value of {entity_name} of the object mention in image and given text information of that image? Give answer in one of the unit from {', '.join(g for  g in entity_unit_map[entity_name])}\"\n",
    "    prompt = f\"USER: <image>\\n {new_prompt} ASSISTANT:\"\n",
    "\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "    # print(inputs['input_ids'].shape)\n",
    "    # print(inputs['attention_mask'].shape)\n",
    "    # print(inputs['pixel_values'].shape)\n",
    "    # print(inputs)\n",
    "    # Generate\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    # Process the response to return only the assistant's answer\n",
    "    if \"ASSISTANT:\" in response:\n",
    "        answer = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    else:\n",
    "        answer = response.strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "inference_from_easyocr_llava('https://m.media-amazon.com/images/I/21BMc5GC4iL.jpg', entity_name = 'width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Custom dataset class for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # Custom dataset class\n",
    "# class ImageEntityDataset(Dataset):\n",
    "#     def __init__(self, dataframe, processor,entity_unit_map, device):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             dataframe (pd.DataFrame): DataFrame with image URLs and entity names.\n",
    "#             processor: Llava processor for preparing inputs.\n",
    "#         \"\"\"\n",
    "#         self.dataframe = dataframe\n",
    "#         self.processor = processor\n",
    "\n",
    "#         self.entity_unit_map = entity_unit_map\n",
    "#         self.device = device\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.dataframe.iloc[idx]\n",
    "#         url = row['image_link']\n",
    "#         entity_name = row['entity_name']\n",
    "#         # print(url)\n",
    "#         # print(entity_name)\n",
    "\n",
    "#         image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "#         # Easy OCR Text form the image\n",
    "#         reader = easyocr.Reader(['en'])  # Specify language\n",
    "#         results = reader.readtext(image)\n",
    "\n",
    "#         new_prompt = 'Text information of image: '\n",
    "\n",
    "#         for (bbox, text, prob) in results:\n",
    "#             new_prompt += text + ' '\n",
    "\n",
    "#         new_prompt += f\"Find the value of {entity_name} of the object mention in image and given text information of that image? Give answer in one of the unit from {', '.join(g for  g in entity_unit_map[entity_name])}\"\n",
    "#         prompt = f\"USER: <image>\\n {new_prompt} ASSISTANT:\"\n",
    "\n",
    "#         # inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#         # Prepare inputs using the processor\n",
    "#         inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "\n",
    "#         return inputs, entity_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Custom collate function to handle batch padding\n",
    "# def collate_fn(batch):\n",
    "#     # Batch images and texts together\n",
    "#     input_texts = [item[0]['input_ids'].squeeze(0) for item in batch]\n",
    "#     attention_masks = [item[0]['attention_mask'].squeeze(0) for item in batch]\n",
    "#     images = [item[0]['pixel_values'].squeeze(0) for item in batch]\n",
    "#     entity_names = [item[1] for item in batch]\n",
    "    \n",
    "#     # Pad the sequences to the same length\n",
    "#     padded_input_texts = torch.nn.utils.rnn.pad_sequence(input_texts, batch_first=True)\n",
    "#     padded_attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True)\n",
    "\n",
    "#     # Stack images as they are already of fixed size\n",
    "#     stacked_images = torch.stack(images, dim=0)\n",
    "\n",
    "#     # Return the inputs in batch form\n",
    "#     return  {'input_ids': padded_input_texts.to(device),\n",
    "#         'attention_mask': padded_attention_masks.to(device),\n",
    "#         'pixel_values': stacked_images.to(device)},  {'entity_names': entity_names}\n",
    "\n",
    "# # DataLoader for faster batching\n",
    "# def create_dataloader(dataframe, processor, entity_unit_map, device, batch_size=8):\n",
    "#     dataset = ImageEntityDataset(dataframe, processor, entity_unit_map, device)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "#     return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have loaded your dataframe and processor\n",
    "# dataloader = create_dataloader(test_data, processor, entity_unit_map, device, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "\n",
    "# for data_dict in dataloader:\n",
    "#     # a , b = data_dict\n",
    "#     # print(data_dict['entity_names'])\n",
    "#     # print(a['input_ids'].shape)\n",
    "#     # print(a['attention_mask'].shape)\n",
    "#     # print(a['pixel_values'].shape)\n",
    "#     print(a)\n",
    "#     print(b)\n",
    "#     c+=1\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_inference(inputs, model):\n",
    "\n",
    "#     input_ids = inputs['input_ids']\n",
    "#     attention_mask = inputs['attention_mask']\n",
    "#     pixel_values = inputs['pixel_values']\n",
    "    \n",
    "#     # Generate responses for the entire batch\n",
    "#     with torch.no_grad():\n",
    "#         generate_ids = model.generate(input_ids=input_ids, \n",
    "#                                       attention_mask=attention_mask, \n",
    "#                                       pixel_values=pixel_values, \n",
    "#                                       max_new_tokens=20)\n",
    "\n",
    "\n",
    "#     # Decode all responses in the batch\n",
    "#     responses = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "#     # Process each response to return only the assistant's answer\n",
    "#     answers = []\n",
    "#     for response in responses:\n",
    "#         if \"ASSISTANT:\" in response:\n",
    "#             answer = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "#         else:\n",
    "#             answer = response.strip()\n",
    "#         answers.append(answer)\n",
    "\n",
    "#     return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for batch in dataloader:\n",
    "#     x, y = batch  # x contains the inputs, y contains entity names\n",
    "\n",
    "#     # Run inference on the batch\n",
    "#     answers = run_inference(x, model)\n",
    "\n",
    "#     # Print or process the answers along with their corresponding entity names\n",
    "#     for entity_name, answer in zip(y['entity_names'], answers):\n",
    "#         print(f\"Entity: {entity_name}, Answer: {answer}\")\n",
    "\n",
    "#     c += 1\n",
    "#     break  # Remove this break to process the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {'index' : [],\n",
    "       'prediction': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['index'][1206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test  = []\n",
    "\n",
    "for i in test_data['index']:\n",
    "    index_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test.index(13719)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = index_test[13708]\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import csv\n",
    "from IPython.display import clear_output\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Open the CSV file in append mode\n",
    "with open('shivam_output.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header row if the file is new (optional)\n",
    "    # Uncomment if you want to write header every time\n",
    "    # writer.writerow([i, 'temp'])\n",
    "\n",
    "    for i in range(13708, 131187):\n",
    "        j = index_test[i]\n",
    "        \n",
    "        try:\n",
    "            image_link = test_data.image_link.values[j]\n",
    "            # Perform OCR inference\n",
    "            temp = inference_from_easyocr_llava(image_link, entity_name=test_data.entity_name.values[j])\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i}: {e}\")\n",
    "            continue  # Skip this iteration if inference fails\n",
    "        \n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([j, [temp]])\n",
    "\n",
    "        print(j, end=\" \")\n",
    "        if  j % 100 == 0:\n",
    "            file.flush()\n",
    "            clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
